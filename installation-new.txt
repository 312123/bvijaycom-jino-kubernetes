create machines like below

node1 --> 2 CPU ,4 GB RAM
node2 --> 2 CPU, 4GB RAM
node3 --> 2 CPU, 4GB RAM
node4 --> 2 CPU, 4GB RAM

node1 is going to be the kubenetes master
node2,node3,node4 going to be nodes

in all the nodes run the following command



Install Docker Engine (run all the nodes)
---------------------

apt-get update && apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -

cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF


apt-get update

apt-get install -y kubelet kubeadm kubectl 

apt-mark hold kubelet kubeadm kubectl 

Install Docker packages (run all the nodes)
---------------------------

apt-get update && apt-get install apt-transport-https ca-certificates curl software-properties-common -y

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -

add-apt-repository \
  "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) \
  stable"

apt-get update && apt-get install docker-ce=18.06.2~ce~3-0~ubuntu -y

cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

Restart docker.
systemctl daemon-reload
systemctl restart docker

*********************************************************************************
run this below command in only node1.coz we are going to make node1 as kube master

kubeadm init --apiserver-advertise-address $(hostname -i) --pod-network-cidr=192.168.0.0/16
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
export kubever=$(kubectl version | base64 | tr -d '\n')

https://www.weave.works/docs/net/latest/kubernetes/kube-addon/ (kube proxy addons installation)


kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"



verify:

kubectl get nodes
kubectl get pods --all-namespaces
kubectl get nodes --show-lables
kubectl get namespaces

Recreate tokens: (if i want to add one more new linux machine then use the below command to add it to the kube cluster)
---------------
kubeadm token create --print-join-command 
(you get some output from this command.
copy that command and paste it to node4 and execute it.
This will help you to add manually any new node to the kube cluster)

****************************

kubectl version
kubectl cluster-info
kubectl get pods -n kube-system
kubectl get events

master convert to node
----------------------

kubectl taint nodes --all node-role.kubernetes.io/master-

***************************************************************************************************************


execute all playbooks in the below manner to get you loud better....

1. create name space

https://github.com/FourTimes/Kubernetes/blob/master/namespace/namespace.yaml

2. create pods...

https://github.com/FourTimes/Kubernetes/blob/master/pod/1-singlepod.yml

kubectl apply -f 1-singlepod.yml
kubectl get all -n twitter -l app=nginx

kubectl exec -it pod/webserver bash -n twitter

now you need to understand that lablels will help you to create exact report and find the particular resources


https://github.com/FourTimes/Kubernetes/blob/master/pod/2-multipods.yml

kubectl get all -n facebook -l app=httpd

kubectl exec -it pod/multicontainer-pods bash -n facebook

kubectl exec -it pod/multicontainer-pods bash -n facebook -c web
kubectl exec -it pod/multicontainer-pods bash -n facebook -c db


https://github.com/bvijaycom/jino-kubernetes/blob/master/pod/2.1-duplicate-port-multipod.yaml

kubectl apply -f 2.1-duplicate-port-multipod.yaml

same port it wont work ---

kubectl describe pod/multicontainer-pods -n facebook

in this only one container will run andother one will give you error

kubectl get events (yuou wont get anything)
kubectl logs pod/multicontainer-pods -n twitter

kubectl logs pod/multicontainer-pods -n twitter -c httpd2
kubectl logs pod/multicontainer-pods -n twitter -c httpd1

-----------------
kubectl create ns twitter
https://github.com/FourTimes/Kubernetes/blob/master/pod/3-storage.yml
kubectl apply -f 3-storage.yml

kubectl get pods -n twitter -o wide

kubectl describe pod/database -n twitter


check the directory has been created to that node

https://github.com/FourTimes/Kubernetes/blob/master/pod/4-multistorage.yml
https://github.com/FourTimes/Kubernetes/blob/master/pod/5-full.yaml
********************************************************************************************
find out the below yaml file is deployed to which name space

https://github.com/FourTimes/Kubernetes/blob/master/pod/6-example.yml

kubectl get pod
kubectl describe pod <pod name>
********************************************************************************************
3. services

https://github.com/FourTimes/Kubernetes/blob/master/service/loadbalancer-facebook.yaml
https://github.com/FourTimes/Kubernetes/blob/master/service/loadbalancer-twiiter.yaml

the above loadbalancer yaml will work only if you have the nodes in google cloud.because you will have the public ip.

https://github.com/FourTimes/Kubernetes/blob/master/service/nodeport-facebook.yaml
https://github.com/FourTimes/Kubernetes/blob/master/service/nodeport-twitter.yaml

kubectl get pod,services -n twitter -o wide

35.227.119.203:32001

access like above from all node machine ip addresses.it should work from all client machine ip address.then the proxy cluster is working good.

kubectl get services
kubectl describe services

4. Replica set

https://github.com/FourTimes/Kubernetes/blob/master/replicaset/1-singlereplica.yml
https://github.com/FourTimes/Kubernetes/blob/master/replicaset/2-singlepod-singlereplica.yml
https://github.com/FourTimes/Kubernetes/blob/master/replicaset/3-multireplica.yaml
https://github.com/FourTimes/Kubernetes/blob/master/replicaset/4-singlepod-singlereplica-error.yaml
https://github.com/FourTimes/Kubernetes/blob/master/replicaset/5-replica-nodeport.yml

kubectl get replicas
kubectl describe replicas

kubectl get pod,service,replicaset -n facebook -o wide

labels --> service -- selector , replica -- match ---> should be same

5. Deployments

https://github.com/FourTimes/Kubernetes/blob/master/deployments/1-deployment-facebook.yaml
https://github.com/FourTimes/Kubernetes/blob/master/deployments/2-deployment-twitter.yml

kubectl get deploy
kubectl describe deploy

kubectl get pod,service,replicaset,deploy -n facebook -o wide

to understand please delete one pod 

kubectl delete pod/nginx-74fcc59689-kn5j2 -n facebook

then 

run the below command again
kubectl get pod,service,replicaset,deploy -n facebook -o wide

rolling update --> https://github.com/bvijaycom/jino-kubernetes/blob/master/deployments/commands.txt


6. dameonsets

https://github.com/bvijaycom/jino-kubernetes/blob/master/daemonset-nginx.yaml

kubectl get all

see now daemonset is 3 running in each client node.because daemonset is mandatory for each worker or client node.

i have only 4 machine including server.so i am doing this test in node3.

kubeadm reset (in node3 give it)

in master run kubectl get all

now you see only daemonset will 2.

kubectl exec -ti pod/nginx-6jlrm bash (loging into one of the pod)



once i configured automatically if i add any linux machine then in that linux machine this ngnix also will be deployed

kubectl get daemonsets
kubectl descibe daemonsets


7.init containers

https://github.com/bvijaycom/jino-kubernetes/blob/master/init/node-redis/node.yml
first deploy this and run kubectl get all command
you will see init is waiting.coz the dependency is not yet launched or stopped
https://github.com/bvijaycom/jino-kubernetes/blob/master/init/node-redis/redis.yml

if we deploy redis.yaml (kubectl apply -f redis.yaml then you run the kubectl get all command you will now see running state)


8. HPA - horizontal pod autoscaling

first execute 

https://github.com/bvijaycom/jino-kubernetes/blob/master/ingress/nginx/deployment.yaml

ingress controller and 3rd party addon will be enabled then run the HPA below


https://github.com/bvijaycom/jino-kubernetes/blob/master/hpa/hpa.yml

9. resource limits
https://github.com/bvijaycom/jino-kubernetes/blob/master/hpa/hpa.yml

kubectl apply -f hpa.yml

kubectl get all 




